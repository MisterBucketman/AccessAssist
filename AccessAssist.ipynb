{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma 2B on Google Colab\n",
    "\n",
    "This notebook is designed to fine-tune the Gemma 2B Instruct model using LoRA (Low-Rank Adaptation) on Google Colab. It uses the Unsloth library for efficient training. This lighter model reduces training time and resource requirements compared to larger models.\n",
    "\n",
    "## Prerequisites:\n",
    "- Google Colab account with GPU enabled (Runtime > Change runtime type > T4 GPU)\n",
    "- Hugging Face account and token\n",
    "- Your dataset in JSON format\n",
    "\n",
    "## Steps:\n",
    "1. Upload your dataset.json to /content/ in Colab\n",
    "2. Set your Hugging Face username and dataset name\n",
    "3. Login to Hugging Face\n",
    "4. Install required packages\n",
    "5. Run the cells sequentially\n",
    "\n",
    "## Important Notes:\n",
    "- Gemma 2B is much lighter than Llama3 8B, requiring less GPU memory and training time.\n",
    "- Training should complete in 10-30 minutes on a T4 GPU with 3 epochs.\n",
    "- Ensure your dataset is in the correct format: list of dicts with 'instruction', 'input', 'output' keys.\n",
    "- Monitor GPU usage; this model should fit comfortably in Colab's free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWrH2GyEFlz6"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install unsloth\n",
    "!pip install transformers datasets accelerate peft trl bitsandbytes\n",
    "!pip install huggingface_hub\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Login to Hugging Face\n",
    "# Replace 'your_hf_token' with your actual token\n",
    "login(\"your_hf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your configurations\n",
    "huggingface_user = \"FieryXcalibur\"  # Your HF username\n",
    "dataset_name = \"your_dataset_name\"  # Name for your dataset on HF\n",
    "hf_token = \"your_hf_token\"  # Your HF token\n",
    "\n",
    "# Login to HF\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6ZzbDEmYna3"
   },
   "outputs": [],
   "source": [
    "huggingface_user = \"FieryXcalibur\"\n",
    "dataset_name = \"your_dataset_name\"  # Set your dataset name\n",
    "\n",
    "class GemmaInstructDataset:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.prompts = []\n",
    "        self.create_prompts()\n",
    "\n",
    "    def create_prompt(self, row):\n",
    "        prompt = f\"\"\"<start_of_turn>user\n",
    "{row['instruction']}\n",
    "{row['input']}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{row['output']}<end_of_turn>\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def create_prompts(self):\n",
    "        for row in self.data:\n",
    "            prompt = self.create_prompt(row)\n",
    "            self.prompts.append(prompt)\n",
    "\n",
    "    def get_dataset(self):\n",
    "        df = pd.DataFrame({'prompt': self.prompts})\n",
    "        return df\n",
    "\n",
    "def create_dataset_hf(dataset):\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    return DatasetDict({\"train\": Dataset.from_pandas(dataset)})\n",
    "\n",
    "# Load and process dataset\n",
    "with open('/content/dataset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dataset = GemmaInstructDataset(data)\n",
    "df = dataset.get_dataset()\n",
    "\n",
    "processed_data_path = 'processed_data'\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "gemma_dataset = create_dataset_hf(df)\n",
    "gemma_dataset.save_to_disk(os.path.join(processed_data_path, \"gemma_dataset\"))\n",
    "gemma_dataset.push_to_hub(f\"{huggingface_user}/{dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_WiE8XYVXqQ"
   },
   "source": [
    "# **Step 5.** LoRa Finetuning Configurations\n",
    "- \"finetuned_model\" sets your models name on HF\n",
    "- \"num_train_epochs\" sets the number of epochs for training\n",
    "\n",
    "    (epoch = 1 pass through your entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQQN-uSUzB8h"
   },
   "outputs": [],
   "source": [
    "# Defining the configuration for the base model, LoRA and training\n",
    "config = {\n",
    "    \"hugging_face_username\":huggingface_user,\n",
    "    \"model_config\": {\n",
    "        \"base_model\":\"unsloth/gemma-2b-it-bnb-4bit\", # Lighter 2B parameter model for faster training\n",
    "        \"finetuned_model\":f\"{huggingface_user}/gemma-2b-accessassist-finetuned\", # The finetuned model\n",
    "        \"max_seq_length\": 1024, # Reduced sequence length for faster training\n",
    "        \"dtype\":torch.float16, # The data type\n",
    "        \"load_in_4bit\": True, # Load the model in 4-bit\n",
    "    },\n",
    "    \"lora_config\": {\n",
    "      \"r\": 8, # Reduced LoRA rank for faster training\n",
    "      \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"], # The target modules\n",
    "      \"lora_alpha\":8, # Adjusted alpha\n",
    "      \"lora_dropout\":0, # The dropout value for LoRA\n",
    "      \"bias\":\"none\", # The bias for LoRA\n",
    "      \"use_gradient_checkpointing\":True, # Use gradient checkpointing\n",
    "      \"use_rslora\":False, # Use RSLora\n",
    "      \"use_dora\":False, # Use DoRa\n",
    "      \"loftq_config\":None # The LoFTQ configuration\n",
    "    },\n",
    "    \"training_dataset\":{\n",
    "        \"name\":f\"{huggingface_user}/{dataset_name}\", # The dataset name(huggingface/datasets)\n",
    "        \"split\":\"train\", # The dataset split\n",
    "        \"input_field\":\"prompt\", # The input field\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"per_device_train_batch_size\": 4, # Increased batch size for smaller model\n",
    "        \"gradient_accumulation_steps\": 2, # Reduced accumulation steps\n",
    "        \"warmup_steps\": 5, # The warmup steps\n",
    "        \"max_steps\":0, # The maximum steps (0 if the epochs are defined)\n",
    "        \"num_train_epochs\": 3, # Reduced epochs for faster training\n",
    "        \"learning_rate\": 2e-4, # The learning rate\n",
    "        \"fp16\": not torch.cuda.is_bf16_supported(),  # The fp16\n",
    "        \"bf16\": torch.cuda.is_bf16_supported(), # The bf16\n",
    "        \"logging_steps\": 1, # The logging steps\n",
    "        \"optim\" :\"adamw_8bit\", # The optimizer\n",
    "        \"weight_decay\" : 0.01,  # The weight decay\n",
    "        \"lr_scheduler_type\": \"linear\", # The learning rate scheduler\n",
    "        \"seed\" : 42, # The seed\n",
    "        \"output_dir\" : \"outputs\", # The output directory\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcpVNM_7ZGbi"
   },
   "source": [
    "# **Step 6.** Load Llama3-8B, QLoRA & Trainer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4wxJAgnM2W0"
   },
   "outputs": [],
   "source": [
    "# Loading the model and the tokinizer for the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = config.get(\"model_config\").get(\"base_model\"),\n",
    "    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
    "    dtype = config.get(\"model_config\").get(\"dtype\"),\n",
    "    load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
    ")\n",
    "\n",
    "# Setup for QLoRA/LoRA peft of the base model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = config.get(\"lora_config\").get(\"r\"),\n",
    "    target_modules = config.get(\"lora_config\").get(\"target_modules\"),\n",
    "    lora_alpha = config.get(\"lora_config\").get(\"lora_alpha\"),\n",
    "    lora_dropout = config.get(\"lora_config\").get(\"lora_dropout\"),\n",
    "    bias = config.get(\"lora_config\").get(\"bias\"),\n",
    "    use_gradient_checkpointing = config.get(\"lora_config\").get(\"use_gradient_checkpointing\"),\n",
    "    random_state = 42,\n",
    "    use_rslora = config.get(\"lora_config\").get(\"use_rslora\"),\n",
    "    use_dora = config.get(\"lora_config\").get(\"use_dora\"),\n",
    "    loftq_config = config.get(\"lora_config\").get(\"loftq_config\"),\n",
    ")\n",
    "\n",
    "# Loading the training dataset\n",
    "dataset_train = load_dataset(config.get(\"training_dataset\").get(\"name\"), split = config.get(\"training_dataset\").get(\"split\"))\n",
    "\n",
    "# Setting up the trainer for the model\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_train,\n",
    "    dataset_text_field = config.get(\"training_dataset\").get(\"input_field\"),\n",
    "    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = config.get(\"training_config\").get(\"per_device_train_batch_size\"),\n",
    "        gradient_accumulation_steps = config.get(\"training_config\").get(\"gradient_accumulation_steps\"),\n",
    "        warmup_steps = config.get(\"training_config\").get(\"warmup_steps\"),\n",
    "        max_steps = config.get(\"training_config\").get(\"max_steps\"),\n",
    "        num_train_epochs= config.get(\"training_config\").get(\"num_train_epochs\"),\n",
    "        learning_rate = config.get(\"training_config\").get(\"learning_rate\"),\n",
    "        fp16 = config.get(\"training_config\").get(\"fp16\"),\n",
    "        bf16 = config.get(\"training_config\").get(\"bf16\"),\n",
    "        logging_steps = config.get(\"training_config\").get(\"logging_steps\"),\n",
    "        optim = config.get(\"training_config\").get(\"optim\"),\n",
    "        weight_decay = config.get(\"training_config\").get(\"weight_decay\"),\n",
    "        lr_scheduler_type = config.get(\"training_config\").get(\"lr_scheduler_type\"),\n",
    "        seed = 42,\n",
    "        output_dir = config.get(\"training_config\").get(\"output_dir\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u-v9ArEnYZb"
   },
   "source": [
    "# **Step 7.** Train Your Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yI9mEQ7ZOUx2"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSeaIIdVnYZb"
   },
   "source": [
    "# **Step 8.** Save Trainer Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWc-3iYuVKpq"
   },
   "outputs": [],
   "source": [
    "with open(\"trainer_stats.json\", \"w\") as f:\n",
    "    json.dump(trainer_stats, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJTKdAmRnYZc"
   },
   "source": [
    "# **Step 9.** Save Finetuned Model & Push to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzDLfiN-VKpq"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method=\"merged_16bit\")\n",
    "model.push_to_hub_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method=\"merged_16bit\")\n",
    "\n",
    "# Optional: Save as GGUF for local inference\n",
    "# model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method=\"q4_k_m\")\n",
    "# model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgyNn-lLnYZc"
   },
   "source": [
    "# **Step 10.** Test your pretrained model in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozVcalyP_JLs"
   },
   "outputs": [],
   "source": [
    "# Loading the fine-tuned model and the tokenizer for inference\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = config.get(\"model_config\").get(\"finetuned_model\"),\n",
    "        max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
    "        dtype = config.get(\"model_config\").get(\"dtype\"),\n",
    "        load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
    "    )\n",
    "\n",
    "# Using FastLanguageModel for fast inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "system_prompt = \"You are an AI task automator. You will take a users prompt and use first principle reasoning to break the prompt into tasks that you must accomplish within another chat. RESPOND TO THIS MESSAGE ONLY WITH A PYTHON FORMATTED LIST OF TASKS THAT YOU MUST COMPLETE TO TRUTHFULLY AND INTELLIGENTLY ACCOMPLISH THE USERS REQUEST. ASSUME YOU CAN SEARCH THE WEB, WRITE CODE, RUN CODE, DEBUG CODE, AND AUTOMATE ANYTHING ON THE USERS COMPUTER TO ACCOMPLISH THE PROMPT. CORRECT RESPONSE FORMAT: ['task 1', 'task 2', 'task 3']\"\n",
    "\n",
    "# Tokenizing the input and generating the output\n",
    "prompt = input('TYPE PROMPT TO GEMMA: ')\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    f\"<start_of_turn>user\\n{system_prompt}\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dcYaYEWORQw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM219A5vanoxTL2wWhgh2fs",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
